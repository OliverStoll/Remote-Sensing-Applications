{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7e91616",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dbfe447f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from simple_downloader import download\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "from torchvision import models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a1524e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define Customized Dataset in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c69358a7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class UCMerced(Dataset):\n",
    "    def __init__(self, root_dir, img_transform=None, multilabel=False):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.images_path = os.path.join(root_dir, \"Images\")\n",
    "        self.class_names = sorted(\n",
    "            [cl for cl in os.listdir(self.images_path) if not cl.startswith(\".\")]\n",
    "        )\n",
    "        self.img_paths, self.img_labels = self.init_dataset()\n",
    "        self.img_transform = img_transform\n",
    "\n",
    "        if multilabel:\n",
    "            self.img_labels = self.read_multilabels()  # important for loss calculation\n",
    "            self.img_labels = self.img_labels.astype(float)\n",
    "\n",
    "    def init_dataset(self):\n",
    "        img_paths, img_labels = [], []\n",
    "        for cl_id, cl_name in enumerate(self.class_names):\n",
    "            cl_path = os.path.join(self.images_path, cl_name)\n",
    "\n",
    "            for img in sorted(os.listdir(cl_path)):\n",
    "                img_path = os.path.join(cl_path, img)\n",
    "                img_paths.append(img_path)\n",
    "                img_labels.append(cl_id)\n",
    "\n",
    "        return img_paths, img_labels\n",
    "\n",
    "    def read_multilabels(self):  # TODO\n",
    "        file_path = self.root_dir + \"/multilabels/LandUse_Multilabeled.xlsx\"\n",
    "\n",
    "        df = pd.read_excel(file_path)\n",
    "        df = df.drop(\"IMAGE\\\\LABEL\", axis=1)\n",
    "        labels_onehot = df.to_numpy()\n",
    "\n",
    "        return labels_onehot\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        label = self.img_labels[idx]\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.img_transform is not None:\n",
    "            img = self.img_transform(img)\n",
    "\n",
    "        return dict(img=img, label=label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fbec3d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define Customized PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f95c34",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "712b0e11",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MetricTracker(object):\n",
    "    \"\"\"Computes and stores the average and current value.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9c3989e6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_device(cuda_int):\n",
    "    \"\"\"Get Cuda-Device. If cuda_int < 0 compute on CPU.\"\"\"\n",
    "    if cuda_int < 0:\n",
    "        print(\"Computation on CPU\")\n",
    "        device = torch.device(\"cpu\")\n",
    "    elif torch.cuda.is_available():\n",
    "        print(\"Computation on CUDA GPU device {}\".format(cuda_int))\n",
    "        device = torch.device(\"cuda:{}\".format(cuda_int))\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "151878ea",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset(root_dir, tr_transform, te_transform, set_sizes, seed=1, multilabel=False):\n",
    "    \"\"\"\n",
    "    Parameter\n",
    "    ---------\n",
    "    root_dir     : path to UCMerced Dataset\n",
    "    tr_transform : transformation for training data\n",
    "    te_transform : transformation for training data\n",
    "    set_sizes    : list of percentage of either train-test or train-val-test (sum to 100)\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    sets for train and test, optionally also val if len(set_sizes)==3\n",
    "    \"\"\"\n",
    "    ucm_dataset_tr = UCMerced(root_dir, img_transform=tr_transform, multilabel=multilabel)\n",
    "    ucm_dataset_te = UCMerced(root_dir, img_transform=te_transform, multilabel=multilabel)\n",
    "    idx_list = split_ucm_indices(set_sizes, seed=seed)\n",
    "\n",
    "    train_set = Subset(ucm_dataset_tr, idx_list[0])\n",
    "    test_set = Subset(ucm_dataset_te, idx_list[-1])\n",
    "\n",
    "    if len(idx_list) > 2:\n",
    "        val_set = Subset(ucm_dataset_te, idx_list[1])\n",
    "        return train_set, val_set, test_set\n",
    "    else:\n",
    "        return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4672edb4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_ucm_indices(set_sizes, num_samples=2100, num_classes=21, seed=1):\n",
    "    \"\"\"Compute indices for a class-balanced train-(val)-test split for UCMerced.\"\"\"\n",
    "    cl_samples = int(num_samples / num_classes)\n",
    "    assert sum(set_sizes) == 100\n",
    "    split_indices = list(map(int, np.cumsum(set_sizes)[:-1] / 100 * cl_samples))\n",
    "    # class_idx_mat d x N (row: classes, columns: idx of sample in dataset)\n",
    "    dataset_idx = np.arange(0, num_samples)\n",
    "    class_idx_mat = np.reshape(dataset_idx, (num_classes, cl_samples))\n",
    "    # random shuffle class_wise idx (=> per row)\n",
    "    np.random.seed(seed)\n",
    "    np.apply_along_axis(np.random.shuffle, 1, class_idx_mat)\n",
    "    # return indices for splits (2 or 3)\n",
    "    idx_list = np.hsplit(class_idx_mat, split_indices)\n",
    "    # flatten set idx\n",
    "    return list(map(lambda x: x.flatten(), idx_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "44443104",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pretty_classification_report_print(report, class_names):\n",
    "    N = len(class_names)\n",
    "    df = pd.DataFrame(report).round(decimals=2)\n",
    "    df = df.rename(columns=dict(zip(list(map(str, range(N))), testset.dataset.class_names))).T\n",
    "    df[[\"support\"]] = df[[\"support\"]].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e2bac4f1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prettify_confusion_matrix(conf_mat, class_names):\n",
    "    plt.subplots(1, 1, figsize=(11, 7))\n",
    "    sns.heatmap(\n",
    "        conf_mat,\n",
    "        cmap=\"viridis\",\n",
    "        fmt=\"g\",\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "        annot=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73bad8d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "79fde830",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, criterion, epochs, device, early_stop=False):\n",
    "    train_losses, val_losses = [], []\n",
    "    accuracy_scores = []\n",
    "    best_model = copy.deepcopy(model)\n",
    "    best_acc = 0\n",
    "    best_epoch = 1\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        print(\"Epoch {}/{}\".format(epoch, epochs))\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, report, _ = val_epoch(model, val_loader, criterion, device)\n",
    "        overall_acc = report[\"accuracy\"]\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        accuracy_scores.append(overall_acc)\n",
    "\n",
    "        if best_acc < overall_acc:\n",
    "            best_acc = overall_acc\n",
    "            best_epoch = epoch\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        if epoch - best_epoch > 10 and early_stop:\n",
    "            break\n",
    "\n",
    "    return best_model, train_losses, val_losses, accuracy_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5ea88b43",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    loss_tracker = MetricTracker()\n",
    "    acc_tracker = MetricTracker()\n",
    "    model.train()\n",
    "\n",
    "    tqdm_bar = tqdm(train_loader, desc=\"Training: \")\n",
    "    for batch in tqdm_bar:\n",
    "\n",
    "        images = batch[\"img\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        batch_size = images.size(0)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        loss_tracker.update(loss.item(), batch_size)\n",
    "\n",
    "        _, predicted = torch.max(probs.data, 1)\n",
    "        batch_acc = (predicted == labels).sum().item() / batch_size\n",
    "        acc_tracker.update(batch_acc, batch_size)\n",
    "        tqdm_bar.set_postfix(loss=loss_tracker.avg, accuracy=acc_tracker.avg)\n",
    "\n",
    "    return loss_tracker.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "15f82316",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def val_epoch(model, val_loader, criterion, device):\n",
    "    loss_tracker = MetricTracker()\n",
    "    acc_tracker = MetricTracker()\n",
    "    model.eval()\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tqdm_bar = tqdm(val_loader, desc=\"Validation: \")\n",
    "        for batch in tqdm_bar:\n",
    "\n",
    "            images = batch[\"img\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            batch_size = images.size(0)\n",
    "\n",
    "            logits = model(images)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss_tracker.update(loss.item(), batch_size)\n",
    "\n",
    "            _, predicted = torch.max(probs.data, 1)\n",
    "            batch_acc = (predicted == labels).sum().item() / batch_size\n",
    "            acc_tracker.update(batch_acc, batch_size)\n",
    "\n",
    "            y_pred += predicted.tolist()\n",
    "            y_true += labels.tolist()\n",
    "            tqdm_bar.set_postfix(loss=loss_tracker.avg, accuracy=acc_tracker.avg)\n",
    "\n",
    "    report = classification_report(y_true, y_pred, zero_division=0, output_dict=True)\n",
    "    conf_mat = confusion_matrix(y_true, y_pred, normalize=\"true\")\n",
    "\n",
    "    return loss_tracker.avg, report, conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b005629d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training on UCMerced Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d518b70",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Download UCMerced Dataset from TUB-Cloud\n",
    "\n",
    "Following workflow from Lab01, creating directory \"./data\", downloading UCMerced dataset zip-file and unzipping it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fb599ff0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "download_dir = Path(\"./data\")\n",
    "download_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "78ea715a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target file already exists!\n",
      "Will skip download. To force download set `force=True`\n"
     ]
    }
   ],
   "source": [
    "TUB_URL = \"https://tubcloud.tu-berlin.de/s/H4QHX5GPDY6wDog/download/UCMerced_LandUse.zip\"\n",
    "output_file = download(TUB_URL, \"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "104033f6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "zipf = zipfile.ZipFile(output_file)\n",
    "zipf.extractall(path=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67db751b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Main Hyperparamter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e149cfd7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation on CUDA GPU device 0\n"
     ]
    }
   ],
   "source": [
    "cuda_device = get_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "aa64a99f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 20\n",
    "num_cls = 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c256fef",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train- and Testset Transformation (i.e., Data Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c588b2fe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ucm_mean = [0.595425, 0.3518577, 0.3225522]\n",
    "ucm_std = [0.19303136, 0.12492529, 0.10577361]\n",
    "\n",
    "tr_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=ucm_mean, std=ucm_std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "te_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=ucm_mean, std=ucm_std),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d354577",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Initializing Train-, Val-, Testset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "62b2a413",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainset, valset, testset = get_dataset(\n",
    "    \"./data/UCMerced_LandUse\",\n",
    "    tr_transform=tr_transform,\n",
    "    te_transform=te_transform,\n",
    "    set_sizes=[70, 10, 20],\n",
    "    multilabel=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c3e8714f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dd4181",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Initialize Model, Loss-Function and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "When using models that are pretrained on a different, usually bigger dataset (e.g. on the popular Computer Vision dataset ImageNet [1]) for a so-called downstream task in which the pretrained model is fine-tuned on the target dataset, we speak about Transfer Learning.\n",
    "\n",
    "In this practice, try to use a pretrained version of the predefined `resnet18` model from Pytorch `models` library. You can achieve this by providing pretrained model weights to the weights parameter at initialization of the model. Use the following weights from the `torchvision` library: `torchvision.models.ResNet18_Weights.DEFAULT`. Finetune it on the train set of UCMerced dataset and evaluate the new model.\n",
    "\n",
    "**Hint**: ImageNet consists of 1000 classes, therefore the last layer of the pretrained model needs to be alternated.\n",
    "\n",
    "for more information check the pytorch documentation:\n",
    "\n",
    "https://pytorch.org/docs/stable/torchvision/models.html\n",
    "\n",
    "[1] https://www.image-net.org/index.php"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "# Add necessary functionality for multi label!!!!\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.ones([17])).to(cuda_device)\n",
    "eval_accuracies = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "74d6056b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "resnet_pretrained = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "resnet_pretrained.fc = nn.Linear(512, num_cls)\n",
    "resnet_pretrained.to(cuda_device)\n",
    "optimizer = optim.SGD(\n",
    "    resnet_pretrained.parameters(),\n",
    "    lr=learning_rate,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0001,\n",
    "    nesterov=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b45c097f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training:   0%|          | 0/23 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e43ce87ada04eab99b7a4619a12d418"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([64, 17])) must be the same as input size (torch.Size([64, 21]))",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [129]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m best_model, train_losses, val_losses, accuracy_scores \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresnet_pretrained\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcuda_device\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m eval_accuracies\u001B[38;5;241m.\u001B[39mappend(accuracy_scores)\n",
      "Input \u001B[1;32mIn [116]\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, train_loader, val_loader, optimizer, criterion, epochs, device, early_stop)\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(epoch, epochs))\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m---> 13\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m val_loss, report, _ \u001B[38;5;241m=\u001B[39m val_epoch(model, val_loader, criterion, device)\n\u001B[0;32m     15\u001B[0m overall_acc \u001B[38;5;241m=\u001B[39m report[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "Input \u001B[1;32mIn [117]\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[1;34m(model, train_loader, optimizer, criterion, device)\u001B[0m\n\u001B[0;32m     12\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     14\u001B[0m logits \u001B[38;5;241m=\u001B[39m model(images)\n\u001B[1;32m---> 15\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mcriterion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     17\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32mC:\\DRIVE\\SOFTWARE\\_Archiv\\_global-venv-pc1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mC:\\DRIVE\\SOFTWARE\\_Archiv\\_global-venv-pc1\\lib\\site-packages\\torch\\nn\\modules\\loss.py:714\u001B[0m, in \u001B[0;36mBCEWithLogitsLoss.forward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m    713\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 714\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbinary_cross_entropy_with_logits\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    715\u001B[0m \u001B[43m                                              \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    716\u001B[0m \u001B[43m                                              \u001B[49m\u001B[43mpos_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpos_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    717\u001B[0m \u001B[43m                                              \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\DRIVE\\SOFTWARE\\_Archiv\\_global-venv-pc1\\lib\\site-packages\\torch\\nn\\functional.py:3148\u001B[0m, in \u001B[0;36mbinary_cross_entropy_with_logits\u001B[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001B[0m\n\u001B[0;32m   3145\u001B[0m     reduction_enum \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mget_enum(reduction)\n\u001B[0;32m   3147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (target\u001B[38;5;241m.\u001B[39msize() \u001B[38;5;241m==\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize()):\n\u001B[1;32m-> 3148\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTarget size (\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m) must be the same as input size (\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(target\u001B[38;5;241m.\u001B[39msize(), \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize()))\n\u001B[0;32m   3150\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mbinary_cross_entropy_with_logits(\u001B[38;5;28minput\u001B[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001B[1;31mValueError\u001B[0m: Target size (torch.Size([64, 17])) must be the same as input size (torch.Size([64, 21]))"
     ]
    }
   ],
   "source": [
    "best_model, train_losses, val_losses, accuracy_scores = train(\n",
    "    resnet_pretrained,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    epochs=epochs,\n",
    "    device=cuda_device,\n",
    ")\n",
    "eval_accuracies.append(accuracy_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}